# ER-CNN-Keras: Emotion Recognition using CNNs with Keras

This repository contains code for an emotion recognition project implemented using Convolutional Neural Networks (CNNs) with the Keras library in Python. The project explores recognizing seven basic human emotions (angry, disgusted, fearful, happy, neutral, sad, surprised) from facial images.

## Project Overview

The primary goal of this project is to build and evaluate models capable of identifying emotions from facial expressions. It includes:

1.  **Custom CNN Model:** A CNN model built from scratch using Keras, trained on the FER-2013 dataset.
2.  **Transfer Learning:** An exploration of using transfer learning with the pre-trained MobileNetV2 model, adapted for the emotion recognition task.
3.  **Real-time Inference:** A script (`test.py`) that uses the trained custom CNN model to detect faces via webcam (using OpenCV and Haar Cascades) and predict their emotions in real-time.
4.  **Documentation:** A detailed PDF report (`Documentation.pdf`) outlining the project context, methodology, results, and comparisons.

## Repository Structure

```
ER-CNN-Keras/
├── Documentation.pdf         # Detailed project report
├── PythonER/                 # Code for the custom CNN model
│   ├── haarcascade/          # Haar cascade file for face detection
│   │   └── haarcascade_frontalface_default.xml
│   ├── script/               # Additional scripts (if any)
│   │   └── script.py
│   ├── test.py               # Real-time emotion recognition script (uses custom CNN)
│   ├── train.py              # Training script for the custom CNN model
│   ├── emodel.json           # Saved custom CNN model architecture (generated by train.py)
│   ├── emodel.h5             # Saved custom CNN model weights (generated by train.py)
│   ├── modelaccuracy.png     # Plot of custom CNN training/validation accuracy
│   ├── modelloss.png         # Plot of custom CNN training/validation loss
│   └── mobilenetv2accuracy.png # Plot related to MobileNetV2 experiments
├── README.md                 # This README file
├── jupyter/                  # Jupyter notebook for transfer learning experiments
│   └── transfer_learning.ipynb
├── poster.pptx               # Project poster presentation
└── teaser.mp4                # Project teaser video
```

## Custom CNN Model

### Architecture

The custom CNN model is defined in `PythonER/train.py` using Keras Sequential API. It consists of the following layers:

*   Input: 48x48 grayscale images
*   Conv2D (32 filters, 3x3 kernel, ReLU)
*   Conv2D (64 filters, 3x3 kernel, ReLU)
*   MaxPooling2D (2x2)
*   Dropout (0.25)
*   Conv2D (128 filters, 3x3 kernel, ReLU)
*   MaxPooling2D (2x2)
*   Conv2D (128 filters, 3x3 kernel, ReLU)
*   MaxPooling2D (2x2)
*   Dropout (0.25)
*   Flatten
*   Dense (1024 units, ReLU)
*   Dropout (0.5)
*   Dense (7 units, Softmax) - Output layer for 7 emotion classes

### Training

*   **Dataset:** FER-2013 (Facial Expression Recognition 2013). Assumes data is organized in `data/train` and `data/test` directories, with subdirectories for each emotion class.
*   **Preprocessing:** Images are rescaled (divided by 255).
*   **Data Augmentation:** `ImageDataGenerator` is used for real-time augmentation during training, including:
    *   Random Rotations (0.1 degrees range)
    *   Random Horizontal Flips
*   **Optimizer:** Adam (learning rate = 0.0001, decay = 1e-6)
*   **Loss Function:** Categorical Crossentropy
*   **Metrics:** Accuracy
*   **Epochs:** 50
*   **Batch Size:** 64
*   **Results:** The model achieved approximately 63.4% accuracy on the test set after 50 epochs (as reported in `Documentation.pdf`).

### Usage (Real-time Recognition)

The `PythonER/test.py` script demonstrates how to use the trained custom CNN model for real-time emotion recognition from a webcam feed.

1.  **Prerequisites:** Ensure you have the necessary libraries installed (OpenCV, Keras/TensorFlow, NumPy).
2.  **Model Files:** Make sure the trained model files (`emodel.json` and `emodel.h5`) and the Haar cascade file (`haarcascade/haarcascade_frontalface_default.xml`) are present in the `PythonER` directory.
3.  **Run the script:**
    ```bash
    cd PythonER
    python test.py
    ```
4.  **Functionality:**
    *   Captures video from the default webcam.
    *   Uses the Haar Cascade classifier to detect faces in each frame.
    *   For each detected face:
        *   Extracts the Region of Interest (ROI).
        *   Converts the ROI to grayscale and resizes it to 48x48 pixels.
        *   Preprocesses the image (normalizes pixel values).
        *   Feeds the image to the loaded CNN model for prediction.
        *   Determines the emotion with the highest probability.
        *   Displays the predicted emotion label above the detected face in the video window.
    *   Press 'q' to quit the application.

## Transfer Learning with MobileNetV2

The `jupyter/transfer_learning.ipynb` notebook explores using MobileNetV2, pre-trained on ImageNet, for emotion recognition.

*   **Approach:**
    1.  Load MobileNetV2 without its top classification layer.
    2.  Freeze the base model's weights.
    3.  Add new layers on top: Data Augmentation (RandomFlip, RandomRotation), GlobalAveragePooling2D, Dropout, and a Dense layer (7 units, Softmax).
    4.  Train only the new layers (feature extraction).
    5.  Optionally, unfreeze some layers of the base model and continue training with a lower learning rate (fine-tuning).
*   **Dataset:** FER-2013, loaded using `tf.keras.utils.image_dataset_from_directory`.
*   **Results:** The documentation reports that this approach (including fine-tuning) achieved around 35% accuracy, which was less convincing than the custom CNN trained from scratch.

## Dependencies

*   Python 3.x
*   TensorFlow / Keras
*   OpenCV-Python (`opencv-python`)
*   NumPy
*   Matplotlib (for plotting in `train.py` and the notebook)
*   Jupyter Notebook (to run the transfer learning experiment)

(Note: Specific versions might be required; refer to potential requirements files if available or test compatibility.)

## References

Please refer to the `Documentation.pdf` file for detailed explanations, methodology, results, and references used in this project.
